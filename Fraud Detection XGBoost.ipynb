{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd3c8962-3951-4ecd-bcd4-2c7acbdf7f18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-3.0.5-py3-none-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\mujta\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from xgboost) (2.2.6)\n",
      "Requirement already satisfied: scipy in c:\\users\\mujta\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from xgboost) (1.16.2)\n",
      "Downloading xgboost-3.0.5-py3-none-win_amd64.whl (56.8 MB)\n",
      "   ---------------------------------------- 0.0/56.8 MB ? eta -:--:--\n",
      "    --------------------------------------- 1.3/56.8 MB 13.4 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 6.8/56.8 MB 26.2 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 12.6/56.8 MB 25.5 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 18.4/56.8 MB 28.3 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 19.1/56.8 MB 21.6 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 20.2/56.8 MB 18.5 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 21.0/56.8 MB 16.2 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 21.8/56.8 MB 14.2 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 26.7/56.8 MB 15.3 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 29.6/56.8 MB 15.4 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 30.4/56.8 MB 14.3 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 31.5/56.8 MB 13.4 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 36.7/56.8 MB 14.5 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 38.0/56.8 MB 13.9 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 40.6/56.8 MB 13.7 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 42.5/56.8 MB 13.4 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 44.6/56.8 MB 13.3 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 46.7/56.8 MB 13.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 48.5/56.8 MB 12.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 50.1/56.8 MB 12.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 51.6/56.8 MB 12.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 53.5/56.8 MB 12.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 55.1/56.8 MB 12.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  56.6/56.8 MB 11.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 56.8/56.8 MB 11.6 MB/s eta 0:00:00\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-3.0.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ip (C:\\Users\\mujta\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (C:\\Users\\mujta\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (C:\\Users\\mujta\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "%pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5978500-9bad-4ebf-846e-202214098e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: imports & configuration\n",
    "import os, io, math, random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image, ImageChops\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "\n",
    "# ML\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve, accuracy_score\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "DATASET_DIR = Path(r\"C:\\Users\\mujta\\OneDrive\\Desktop\\dataset\")   # <- change if needed; expects dataset/real and dataset/fake\n",
    "OUTPUT_DIR = Path(\"./outputs\"); OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "IMAGE_SIZE = (224, 224)   # used for CNN embedding/transfer learning\n",
    "ELA_QUALITY = 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50d86427-7ddb-4469-8cd5-e9c200a89729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: feature extraction helpers\n",
    "import scipy.stats as stats\n",
    "\n",
    "def load_rgb(path, size=None):\n",
    "    img = cv2.imread(str(path))\n",
    "    if img is None:\n",
    "        raise ValueError(f\"Cannot open: {path}\")\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    if size is not None:\n",
    "        img = cv2.resize(img, size, interpolation=cv2.INTER_AREA)\n",
    "    return img\n",
    "\n",
    "def compute_ela_pil(img_rgb, quality=ELA_QUALITY):\n",
    "    # img_rgb: numpy RGB image uint8\n",
    "    pil = Image.fromarray(img_rgb)\n",
    "    buf = io.BytesIO()\n",
    "    pil.save(buf, format='JPEG', quality=quality)\n",
    "    buf.seek(0)\n",
    "    compressed = Image.open(buf).convert(\"RGB\")\n",
    "    ela = ImageChops.difference(pil.convert(\"RGB\"), compressed)\n",
    "    # amplify to full 0-255\n",
    "    extrema = ela.getextrema()\n",
    "    max_diff = max([e[1] for e in extrema]) or 1\n",
    "    scale = 255.0 / max_diff\n",
    "    ela = ela.point(lambda i: int(min(255, i * scale)))\n",
    "    ela_np = np.asarray(ela)\n",
    "    return ela_np\n",
    "\n",
    "def highpass_cv2(img_rgb):\n",
    "    gray = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2GRAY).astype(np.float32)\n",
    "    lap = cv2.Laplacian(gray, cv2.CV_32F)\n",
    "    lap_abs = np.abs(lap)\n",
    "    lap_norm = np.uint8(255 * lap_abs / (lap_abs.max() + 1e-9))\n",
    "    kernel = np.array([[-1,-1,-1],[-1,8,-1],[-1,-1,-1]], np.float32)\n",
    "    hp = cv2.filter2D(gray, -1, kernel)\n",
    "    hp_abs = np.abs(hp)\n",
    "    hp_norm = np.uint8(255 * hp_abs / (hp_abs.max() + 1e-9))\n",
    "    return np.maximum(lap_norm, hp_norm)\n",
    "\n",
    "def fft_features(img_rgb, top_k=20):\n",
    "    gray = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2GRAY).astype(np.float32)\n",
    "    f = np.fft.fft2(gray)\n",
    "    fshift = np.fft.fftshift(f)\n",
    "    magnitude = np.abs(fshift)\n",
    "    log_mag = np.log1p(magnitude)\n",
    "    mean = log_mag.mean()\n",
    "    std = log_mag.std()\n",
    "    skew = float(stats.skew(log_mag.reshape(-1)))\n",
    "    kurt = float(stats.kurtosis(log_mag.reshape(-1)))\n",
    "    h, w = log_mag.shape\n",
    "    cy, cx = h//2, w//2\n",
    "    r = min(cy, cx)\n",
    "    Y, X = np.ogrid[:h, :w]\n",
    "    dist = np.sqrt((Y - cy)**2 + (X - cx)**2)\n",
    "    low_mask = dist <= (0.1*r)\n",
    "    mid_mask = (dist > (0.1*r)) & (dist <= (0.4*r))\n",
    "    high_mask = dist > (0.4*r)\n",
    "    low_e = log_mag[low_mask].sum()/(low_mask.sum()+1e-9)\n",
    "    mid_e = log_mag[mid_mask].sum()/(mid_mask.sum()+1e-9)\n",
    "    high_e = log_mag[high_mask].sum()/(high_mask.sum()+1e-9)\n",
    "    flat = log_mag.reshape(-1)\n",
    "    topk = np.sort(flat)[-top_k:].sum()\n",
    "    return np.array([mean, std, skew, kurt, low_e, mid_e, high_e, topk], dtype=np.float32)\n",
    "\n",
    "# single-image feature extractor combining ELA, HP, FFT, simple stats\n",
    "def extract_handcrafted_features(img_rgb, use_ela=True):\n",
    "    feats = []\n",
    "    if use_ela:\n",
    "        try:\n",
    "            ela = compute_ela_pil(img_rgb)\n",
    "            ela_gray = cv2.cvtColor(ela, cv2.COLOR_RGB2GRAY) if ela.ndim==3 else ela\n",
    "            feats += [float(ela_gray.mean()), float(ela_gray.std()), float(np.percentile(ela_gray,90))]\n",
    "        except Exception as e:\n",
    "            feats += [0.0, 0.0, 0.0]\n",
    "    hp = highpass_cv2(img_rgb)\n",
    "    feats += [float(hp.mean()), float(hp.std()), float(np.percentile(hp,90))]\n",
    "    fftf = fft_features(img_rgb)\n",
    "    feats = np.concatenate([np.array(feats, dtype=np.float32), fftf])\n",
    "    return feats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63813fb8-a8aa-4aaa-ad07-389b8f921d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded ResNet50 for embeddings.\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: optional CNN embeddings using TensorFlow Keras (fast if you have GPU)\n",
    "HAS_TF = False\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "    from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "    from tensorflow.keras.models import Model\n",
    "    HAS_TF = True\n",
    "except Exception as e:\n",
    "    print(\"TensorFlow not available or failed to import. CNN embeddings disabled.\", e)\n",
    "\n",
    "cnn_embedding_model = None\n",
    "if HAS_TF:\n",
    "    base = ResNet50(weights='imagenet', include_top=False, input_shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3))\n",
    "    x = GlobalAveragePooling2D()(base.output)\n",
    "    cnn_embedding_model = Model(inputs=base.input, outputs=x)\n",
    "    print(\"Loaded ResNet50 for embeddings.\")\n",
    "\n",
    "def cnn_embed(img_rgb):\n",
    "    if not HAS_TF or cnn_embedding_model is None:\n",
    "        return np.zeros(2048, dtype=np.float32)\n",
    "    img = cv2.resize(img_rgb, IMAGE_SIZE)\n",
    "    arr = np.expand_dims(img.astype(np.float32), 0)\n",
    "    arr = preprocess_input(arr)\n",
    "    emb = cnn_embedding_model.predict(arr, verbose=0)\n",
    "    return emb.reshape(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5531d34e-cdc9-4593-994e-0ffd15126da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real 475 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing real: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 475/475 [01:12<00:00,  6.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fake 475 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing fake: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 475/475 [01:10<00:00,  6.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix: (950, 2062) Labels: (950,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: build features and labels from dataset/real and dataset/fake\n",
    "def build_features(dataset_dir=DATASET_DIR, use_cnn=HAS_TF, max_per_class=None, verbose=True):\n",
    "    X, y, paths = [], [], []\n",
    "    for label_name, label_idx in [('real',0), ('fake',1)]:\n",
    "        folder = dataset_dir / label_name\n",
    "        if not folder.exists():\n",
    "            raise FileNotFoundError(f\"Expected folder: {folder}\")\n",
    "        files = [p for p in folder.iterdir() if p.suffix.lower() in ('.jpg','.jpeg','.png')]\n",
    "        if max_per_class:\n",
    "            files = files[:max_per_class]\n",
    "        if verbose:\n",
    "            print(label_name, len(files), \"images\")\n",
    "        for p in tqdm(files, desc=f\"Processing {label_name}\"):\n",
    "            try:\n",
    "                img = load_rgb(p, size=IMAGE_SIZE)  # resize to IMAGE_SIZE for speed; FFT works fine\n",
    "                hf = extract_handcrafted_features(img, use_ela=True)\n",
    "                if use_cnn:\n",
    "                    emb = cnn_embed(img)\n",
    "                    feats = np.concatenate([hf, emb])\n",
    "                else:\n",
    "                    feats = hf\n",
    "                X.append(feats)\n",
    "                y.append(label_idx)\n",
    "                paths.append(str(p))\n",
    "            except Exception as e:\n",
    "                print(\"Error:\", p, e)\n",
    "    X = np.vstack(X).astype(np.float32)\n",
    "    y = np.array(y, dtype=np.int32)\n",
    "    return X, y, paths\n",
    "\n",
    "# Run feature building\n",
    "X, y, paths = build_features(DATASET_DIR, use_cnn=HAS_TF, max_per_class=None, verbose=True)\n",
    "print(\"Feature matrix:\", X.shape, \"Labels:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2cd2463a-57ac-42fd-a135-47b63619d855",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RandomForest...\n",
      "ROC AUC: 0.41036011080332413\n",
      "Accuracy: 0.4421052631578947\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        real       0.44      0.43      0.44        95\n",
      "        fake       0.44      0.45      0.45        95\n",
      "\n",
      "    accuracy                           0.44       190\n",
      "   macro avg       0.44      0.44      0.44       190\n",
      "weighted avg       0.44      0.44      0.44       190\n",
      "\n",
      "Confusion matrix:\n",
      " [[41 54]\n",
      " [52 43]]\n",
      "Saved RF model to outputs/rf_detector.joblib\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: train RandomForest classifier on features\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=RANDOM_STATE)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "clf = RandomForestClassifier(n_estimators=250, class_weight='balanced', random_state=RANDOM_STATE, n_jobs=-1)\n",
    "pipe = Pipeline([('scaler', scaler), ('clf', clf)])\n",
    "\n",
    "print(\"Training RandomForest...\")\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = pipe.predict(X_test)\n",
    "try:\n",
    "    y_proba = pipe.predict_proba(X_test)[:,1]\n",
    "    auc = roc_auc_score(y_test, y_proba)\n",
    "    print(\"ROC AUC:\", auc)\n",
    "except Exception:\n",
    "    y_proba = None\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred, target_names=['real','fake']))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Save\n",
    "joblib.dump(pipe, OUTPUT_DIR/\"rf_detector.joblib\")\n",
    "print(\"Saved RF model to outputs/rf_detector.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05948e42-c46f-4d89-a535-d53d544f0db2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 760 images belonging to 2 classes.\n",
      "Found 190 images belonging to 2 classes.\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
      "\u001b[1m9406464/9406464\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
      "Training MobileNetV2 (frozen base)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mujta\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "\u001b[1m48/48\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 270ms/step - accuracy: 0.4585 - loss: 0.8521"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m48/48\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 387ms/step - accuracy: 0.4590 - loss: 0.8513 - val_accuracy: 0.4316 - val_loss: 0.7571\n",
      "Epoch 2/12\n",
      "\u001b[1m48/48\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 384ms/step - accuracy: 0.5681 - loss: 0.7090 - val_accuracy: 0.5053 - val_loss: 0.7976\n",
      "Epoch 3/12\n",
      "\u001b[1m48/48\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 395ms/step - accuracy: 0.5550 - loss: 0.7263 - val_accuracy: 0.4789 - val_loss: 0.7763\n",
      "Epoch 4/12\n",
      "\u001b[1m48/48\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 313ms/step - accuracy: 0.5760 - loss: 0.6895 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m48/48\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 404ms/step - accuracy: 0.5757 - loss: 0.6897 - val_accuracy: 0.4632 - val_loss: 0.7434\n",
      "Epoch 5/12\n",
      "\u001b[1m48/48\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 417ms/step - accuracy: 0.5591 - loss: 0.7075 - val_accuracy: 0.5211 - val_loss: 0.7516\n",
      "Epoch 6/12\n",
      "\u001b[1m48/48\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 313ms/step - accuracy: 0.5923 - loss: 0.6953 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m48/48\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 404ms/step - accuracy: 0.5923 - loss: 0.6950 - val_accuracy: 0.5105 - val_loss: 0.7384\n",
      "Epoch 7/12\n",
      "\u001b[1m48/48\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 405ms/step - accuracy: 0.5498 - loss: 0.6802 - val_accuracy: 0.5000 - val_loss: 0.8018\n",
      "Epoch 8/12\n",
      "\u001b[1m48/48\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 405ms/step - accuracy: 0.6032 - loss: 0.6578 - val_accuracy: 0.5211 - val_loss: 0.8440\n",
      "Epoch 9/12\n",
      "\u001b[1m48/48\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 381ms/step - accuracy: 0.5970 - loss: 0.6421 - val_accuracy: 0.4737 - val_loss: 0.7614\n",
      "Epoch 10/12\n",
      "\u001b[1m48/48\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 308ms/step - accuracy: 0.5471 - loss: 0.6811 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m48/48\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 400ms/step - accuracy: 0.5479 - loss: 0.6806 - val_accuracy: 0.5211 - val_loss: 0.7345\n",
      "Epoch 11/12\n",
      "\u001b[1m48/48\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 409ms/step - accuracy: 0.6034 - loss: 0.6549 - val_accuracy: 0.5211 - val_loss: 0.7868\n",
      "Epoch 12/12\n",
      "\u001b[1m48/48\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 403ms/step - accuracy: 0.6424 - loss: 0.6226 - val_accuracy: 0.5211 - val_loss: 0.8054\n",
      "Fine-tuning last layers...\n",
      "Epoch 1/12\n",
      "\u001b[1m48/48\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 514ms/step - accuracy: 0.5257 - loss: 0.7788 - val_accuracy: 0.4632 - val_loss: 0.7357\n",
      "Epoch 2/12\n",
      "\u001b[1m48/48\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 470ms/step - accuracy: 0.5660 - loss: 0.7148 - val_accuracy: 0.4684 - val_loss: 0.7422\n",
      "Epoch 3/12\n",
      "\u001b[1m48/48\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 501ms/step - accuracy: 0.6059 - loss: 0.6870 - val_accuracy: 0.5105 - val_loss: 0.7409\n",
      "Epoch 4/12\n",
      "\u001b[1m48/48\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 528ms/step - accuracy: 0.6086 - loss: 0.6529 - val_accuracy: 0.5000 - val_loss: 0.7481\n",
      "Epoch 5/12\n",
      "\u001b[1m48/48\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 486ms/step - accuracy: 0.5493 - loss: 0.7145 - val_accuracy: 0.4895 - val_loss: 0.7582\n",
      "Epoch 6/12\n",
      "\u001b[1m48/48\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 487ms/step - accuracy: 0.6334 - loss: 0.6407 - val_accuracy: 0.5053 - val_loss: 0.7578\n",
      "Epoch 7/12\n",
      "\u001b[1m48/48\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 470ms/step - accuracy: 0.6485 - loss: 0.6115 - val_accuracy: 0.5211 - val_loss: 0.7662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved MobileNet model to outputs/mobilenet_final.h5\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Transfer learning model (MobileNetV2). Requires TensorFlow.\n",
    "if HAS_TF:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "    from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input\n",
    "    from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "    from tensorflow.keras.models import Model\n",
    "    from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "    # Create simple ImageDataGenerators using folders (will use DATASET_DIR with subfolders real/fake)\n",
    "    batch_size = 16\n",
    "    train_gen = ImageDataGenerator(preprocessing_function=preprocess_input,\n",
    "                                   validation_split=0.2,\n",
    "                                   rotation_range=5,\n",
    "                                   width_shift_range=0.02,\n",
    "                                   height_shift_range=0.02,\n",
    "                                   brightness_range=(0.9,1.1),\n",
    "                                   horizontal_flip=False)\n",
    "\n",
    "    train_flow = train_gen.flow_from_directory(str(DATASET_DIR), target_size=IMAGE_SIZE,\n",
    "                                               batch_size=batch_size, class_mode='binary', subset='training', shuffle=True, seed=RANDOM_STATE)\n",
    "    val_flow = train_gen.flow_from_directory(str(DATASET_DIR), target_size=IMAGE_SIZE,\n",
    "                                             batch_size=batch_size, class_mode='binary', subset='validation', shuffle=False, seed=RANDOM_STATE)\n",
    "\n",
    "    base = MobileNetV2(weights='imagenet', include_top=False, input_shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3))\n",
    "    x = GlobalAveragePooling2D()(base.output)\n",
    "    x = Dropout(0.3)(x)\n",
    "    out = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(base.input, out)\n",
    "\n",
    "    # Freeze base first\n",
    "    for layer in base.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(1e-3), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    ckpt = ModelCheckpoint(str(OUTPUT_DIR/'mobilenet_finetune.h5'), save_best_only=True, monitor='val_loss')\n",
    "    es = EarlyStopping(patience=6, restore_best_weights=True)\n",
    "\n",
    "    steps = math.ceil(train_flow.samples / batch_size)\n",
    "    vsteps = math.ceil(val_flow.samples / batch_size)\n",
    "    print(\"Training MobileNetV2 (frozen base)...\")\n",
    "    model.fit(train_flow, epochs=12, validation_data=val_flow, steps_per_epoch=steps, validation_steps=vsteps, callbacks=[ckpt, es])\n",
    "\n",
    "    # Unfreeze last blocks and fine-tune\n",
    "    for layer in base.layers[-40:]:\n",
    "        layer.trainable = True\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    print(\"Fine-tuning last layers...\")\n",
    "    model.fit(train_flow, epochs=12, validation_data=val_flow, steps_per_epoch=steps, validation_steps=vsteps, callbacks=[ckpt, es])\n",
    "\n",
    "    # Save final model\n",
    "    model.save(str(OUTPUT_DIR/'mobilenet_final.h5'))\n",
    "    print(\"Saved MobileNet model to outputs/mobilenet_final.h5\")\n",
    "\n",
    "else:\n",
    "    print(\"TensorFlow not installed, skipping transfer-learning CNN.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "102a6b48-cad3-4685-9bed-f76c29d49a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample path: C:\\Users\\mujta\\OneDrive\\Desktop\\00000888_in.jpg\n",
      "RF prediction: real prob(fake)= 0.476\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: ensemble / predict helper\n",
    "def predict_image_with_rf(img_path, rf_pipe=pipe):\n",
    "    img = load_rgb(img_path, size=IMAGE_SIZE)\n",
    "    feats = extract_handcrafted_features(img, use_ela=True)\n",
    "    if HAS_TF:\n",
    "        emb = cnn_embed(img)\n",
    "        feats = np.concatenate([feats, emb])\n",
    "    feats = feats.reshape(1, -1)\n",
    "    pred = rf_pipe.predict(feats)[0]\n",
    "    proba = rf_pipe.predict_proba(feats)[0,1]\n",
    "    return pred, proba\n",
    "\n",
    "# Example:\n",
    "sample = Path(r\"C:\\Users\\mujta\\OneDrive\\Desktop\\00000888_in.jpg\")\n",
    "print(\"Sample path:\", sample)\n",
    "p, prob = predict_image_with_rf(sample)\n",
    "print(\"RF prediction:\", \"fake\" if p==1 else \"real\", \"prob(fake)=\", prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2b7ae3b-f1cf-41a2-b76e-b7b6dbb46651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing real images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 475/475 [01:00<00:00,  7.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing fake images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 475/475 [01:13<00:00,  6.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final feature matrix: (950, 2057)\n",
      "\n",
      "--- Evaluation ---\n",
      "ROC AUC: 0.6072096917167339\n",
      "Accuracy: 0.5508771929824562\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.59      0.57       143\n",
      "           1       0.55      0.51      0.53       142\n",
      "\n",
      "    accuracy                           0.55       285\n",
      "   macro avg       0.55      0.55      0.55       285\n",
      "weighted avg       0.55      0.55      0.55       285\n",
      "\n",
      "Confusion matrix:\n",
      "[[84 59]\n",
      " [69 73]]\n",
      "\n",
      "âœ… Saved model to outputs/fraud_detector.joblib\n"
     ]
    }
   ],
   "source": [
    "# ======================================================\n",
    "# Brazilian ID Fraud Detection (CPU-friendly)\n",
    "# Features: ELA + FFT + High-pass + ResNet50 embeddings\n",
    "# Classifier: Logistic Regression\n",
    "# ======================================================\n",
    "\n",
    "import os, io, cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, roc_auc_score\n",
    ")\n",
    "\n",
    "# TensorFlow / Keras for ResNet\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# PIL for ELA\n",
    "from PIL import Image, ImageChops\n",
    "\n",
    "# -----------------------------\n",
    "# Paths\n",
    "# -----------------------------\n",
    "DATASET_DIR = Path(r\"C:\\Users\\mujta\\OneDrive\\Desktop\\dataset\")   # Change if needed\n",
    "real_dir = Path(DATASET_DIR) / \"real\"\n",
    "fake_dir = Path(DATASET_DIR) / \"fake\"\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "\n",
    "# -----------------------------\n",
    "# Feature extractors\n",
    "# -----------------------------\n",
    "\n",
    "def extract_ela_features(img_path, quality=90, size=(128,128)):\n",
    "    try:\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "    except:\n",
    "        return np.zeros(3)\n",
    "\n",
    "    # Save to JPEG in memory\n",
    "    buf = io.BytesIO()\n",
    "    img.save(buf, \"JPEG\", quality=quality)\n",
    "    buf.seek(0)\n",
    "    compressed = Image.open(buf)\n",
    "\n",
    "    ela_img = ImageChops.difference(img, compressed)\n",
    "    ela_img = ela_img.resize(size)\n",
    "    ela_gray = np.array(ela_img.convert(\"L\"))\n",
    "\n",
    "    return np.array([\n",
    "        ela_gray.mean(),\n",
    "        ela_gray.std(),\n",
    "        np.percentile(ela_gray, 95)\n",
    "    ], dtype=np.float32)\n",
    "\n",
    "def extract_fft_features(img_path, size=(128,128)):\n",
    "    try:\n",
    "        img = cv2.imread(str(img_path), cv2.IMREAD_GRAYSCALE)\n",
    "        img = cv2.resize(img, size)\n",
    "    except:\n",
    "        return np.zeros(3)\n",
    "\n",
    "    f = np.fft.fft2(img)\n",
    "    fshift = np.fft.fftshift(f)\n",
    "    magnitude = 20*np.log(np.abs(fshift) + 1)\n",
    "\n",
    "    return np.array([\n",
    "        magnitude.mean(),\n",
    "        magnitude.std(),\n",
    "        np.percentile(magnitude, 95)\n",
    "    ], dtype=np.float32)\n",
    "\n",
    "def extract_highpass_features(img_path, size=(128,128)):\n",
    "    try:\n",
    "        img = cv2.imread(str(img_path), cv2.IMREAD_GRAYSCALE)\n",
    "        img = cv2.resize(img, size)\n",
    "    except:\n",
    "        return np.zeros(3)\n",
    "\n",
    "    lap = cv2.Laplacian(img, cv2.CV_64F)\n",
    "    return np.array([\n",
    "        lap.mean(),\n",
    "        lap.std(),\n",
    "        np.percentile(lap, 95)\n",
    "    ], dtype=np.float32)\n",
    "\n",
    "# -----------------------------\n",
    "# ResNet50 feature extractor\n",
    "# -----------------------------\n",
    "base_model = ResNet50(weights=\"imagenet\", include_top=False, pooling=\"avg\")\n",
    "model = Model(inputs=base_model.input, outputs=base_model.output)\n",
    "\n",
    "def extract_resnet_embedding(img_path, target_size=(224,224)):\n",
    "    try:\n",
    "        img = image.load_img(img_path, target_size=target_size)\n",
    "        x = image.img_to_array(img)\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        x = preprocess_input(x)\n",
    "        feat = model.predict(x, verbose=0)\n",
    "        return feat.flatten()\n",
    "    except:\n",
    "        return np.zeros(model.output_shape[1], dtype=np.float32)\n",
    "\n",
    "# -----------------------------\n",
    "# Build dataset\n",
    "# -----------------------------\n",
    "X, y = [], []\n",
    "\n",
    "def process_dir(directory, label):\n",
    "    for p in tqdm(list(Path(directory).glob(\"*\"))):\n",
    "        try:\n",
    "            f_ela = extract_ela_features(p)\n",
    "            f_fft = extract_fft_features(p)\n",
    "            f_hp  = extract_highpass_features(p)\n",
    "            f_res = extract_resnet_embedding(p)\n",
    "            features = np.concatenate([f_ela, f_fft, f_hp, f_res])\n",
    "            X.append(features)\n",
    "            y.append(label)\n",
    "        except Exception as e:\n",
    "            print(\"âš ï¸ Error with:\", p, e)\n",
    "\n",
    "print(\"Processing real images...\")\n",
    "process_dir(real_dir, 0)\n",
    "print(\"Processing fake images...\")\n",
    "process_dir(fake_dir, 1)\n",
    "\n",
    "X, y = np.array(X), np.array(y)\n",
    "print(\"Final feature matrix:\", X.shape)\n",
    "\n",
    "# -----------------------------\n",
    "# Train/test split\n",
    "# -----------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Train classifier\n",
    "# -----------------------------\n",
    "clf = LogisticRegression(max_iter=1000, n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# -----------------------------\n",
    "# Evaluate\n",
    "# -----------------------------\n",
    "y_pred = clf.predict(X_test)\n",
    "y_proba = clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "print(\"\\n--- Evaluation ---\")\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_proba))\n",
    "print(\"Accuracy:\", clf.score(X_test, y_test))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# -----------------------------\n",
    "# Save model\n",
    "# -----------------------------\n",
    "joblib.dump(clf, \"outputs/fraud_detector.joblib\")\n",
    "print(\"\\nâœ… Saved model to outputs/fraud_detector.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fee18fed-9ecc-463f-8152-e87d37f67c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 0 images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 475/475 [00:45<00:00, 10.40it/s]\n",
      "Processing 1 images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 475/475 [00:52<00:00,  9.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real: (475, 49674) Fake: (475, 49674)\n",
      "Final feature matrix: (950, 49674)\n",
      "\n",
      "--- Evaluation (XGBoost) ---\n",
      "ROC AUC: 0.4576972323451196\n",
      "Accuracy: 0.4666666666666667\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.48      0.48       143\n",
      "           1       0.46      0.45      0.46       142\n",
      "\n",
      "    accuracy                           0.47       285\n",
      "   macro avg       0.47      0.47      0.47       285\n",
      "weighted avg       0.47      0.47      0.47       285\n",
      "\n",
      "Confusion matrix:\n",
      "[[69 74]\n",
      " [78 64]]\n",
      "\n",
      "âœ… Saved XGBoost model to outputs/xgb_detector.joblib\n"
     ]
    }
   ],
   "source": [
    "# ======================================================\n",
    "# Brazilian ID Fraud Detection - Full Pipeline (Fixed)\n",
    "# ======================================================\n",
    "\n",
    "import os, cv2, io\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from PIL import Image, ImageChops, ImageEnhance\n",
    "\n",
    "# ML\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "import joblib\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Torch for embeddings\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# -----------------------------\n",
    "# Config\n",
    "# -----------------------------\n",
    "dataset_path = Path(r\"C:\\Users\\mujta\\OneDrive\\Desktop\\dataset\")   # inside: dataset/real , dataset/fake\n",
    "output_dir = \"outputs\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "IMG_SIZE = (128, 128)  # fixed size for feature extraction\n",
    "\n",
    "# -----------------------------\n",
    "# Image Processing Functions\n",
    "# -----------------------------\n",
    "def ela_features(image_path, quality=90):\n",
    "    \"\"\"Error Level Analysis with fixed size\"\"\"\n",
    "    image = Image.open(image_path).convert(\"RGB\").resize(IMG_SIZE)\n",
    "    buffer = io.BytesIO()\n",
    "    image.save(buffer, \"JPEG\", quality=quality)\n",
    "    compressed = Image.open(buffer)\n",
    "    ela_img = ImageChops.difference(image, compressed)\n",
    "    extrema = ela_img.getextrema()\n",
    "    max_diff = max([ex[1] for ex in extrema])\n",
    "    scale = 255.0 / max_diff if max_diff != 0 else 1\n",
    "    ela_img = ImageEnhance.Brightness(ela_img).enhance(scale)\n",
    "    return np.array(ela_img).flatten()\n",
    "\n",
    "def fft_features(image_path):\n",
    "    \"\"\"FFT magnitude spectrum stats (fixed size)\"\"\"\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if img is None: \n",
    "        return np.zeros(5)\n",
    "    img = cv2.resize(img, IMG_SIZE)\n",
    "    f = np.fft.fft2(img)\n",
    "    fshift = np.fft.fftshift(f)\n",
    "    magnitude = 20*np.log(np.abs(fshift) + 1)\n",
    "    return np.array([\n",
    "        np.mean(magnitude),\n",
    "        np.std(magnitude),\n",
    "        np.median(magnitude),\n",
    "        np.max(magnitude),\n",
    "        np.min(magnitude)\n",
    "    ])\n",
    "\n",
    "def highpass_features(image_path):\n",
    "    \"\"\"High-pass filter features (fixed size)\"\"\"\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if img is None: \n",
    "        return np.zeros(5)\n",
    "    img = cv2.resize(img, IMG_SIZE)\n",
    "    dft = cv2.dft(np.float32(img), flags=cv2.DFT_COMPLEX_OUTPUT)\n",
    "    dft_shift = np.fft.fftshift(dft)\n",
    "    rows, cols = img.shape\n",
    "    crow, ccol = rows//2, cols//2\n",
    "    mask = np.ones((rows, cols, 2), np.uint8)\n",
    "    mask[crow-10:crow+10, ccol-10:ccol+10] = 0\n",
    "    fshift = dft_shift * mask\n",
    "    f_ishift = np.fft.ifftshift(fshift)\n",
    "    img_back = cv2.idft(f_ishift)\n",
    "    img_back = cv2.magnitude(img_back[:,:,0], img_back[:,:,1])\n",
    "    return np.array([\n",
    "        np.mean(img_back),\n",
    "        np.std(img_back),\n",
    "        np.median(img_back),\n",
    "        np.max(img_back),\n",
    "        np.min(img_back)\n",
    "    ])\n",
    "\n",
    "# -----------------------------\n",
    "# ResNet Embeddings\n",
    "# -----------------------------\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "resnet = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "resnet.fc = torch.nn.Identity()  # remove classifier\n",
    "resnet = resnet.to(device)\n",
    "resnet.eval()\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "def deep_features(image_path):\n",
    "    img = Image.open(image_path).convert(\"RGB\").resize((224,224))\n",
    "    tensor = transform(img).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        feat = resnet(tensor).cpu().numpy().flatten()\n",
    "    return feat\n",
    "\n",
    "# -----------------------------\n",
    "# Feature Extraction\n",
    "# -----------------------------\n",
    "def extract_features(folder, label):\n",
    "    features, labels = [], []\n",
    "    files = list(Path(folder).glob(\"*_in.jpg\"))\n",
    "    if len(files) == 0:\n",
    "        print(f\"âš ï¸ No images found in {folder}\")\n",
    "        return np.empty((0,)), np.empty((0,))\n",
    "\n",
    "    for f in tqdm(files, desc=f\"Processing {label} images\"):\n",
    "        f = str(f)\n",
    "        try:\n",
    "            feat_ela = ela_features(f)\n",
    "            feat_fft = fft_features(f)\n",
    "            feat_hp  = highpass_features(f)\n",
    "            feat_res = deep_features(f)\n",
    "            all_feat = np.concatenate([feat_ela, feat_fft, feat_hp, feat_res])\n",
    "            features.append(all_feat)\n",
    "            labels.append(label)\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Skipped {f}: {e}\")\n",
    "            continue\n",
    "\n",
    "    return np.array(features), np.array(labels)\n",
    "\n",
    "# -----------------------------\n",
    "# Load Dataset\n",
    "# -----------------------------\n",
    "X_real, y_real = extract_features(os.path.join(dataset_path, \"real\"), 0)\n",
    "X_fake, y_fake = extract_features(os.path.join(dataset_path, \"fake\"), 1)\n",
    "\n",
    "print(\"Real:\", X_real.shape, \"Fake:\", X_fake.shape)\n",
    "\n",
    "X = np.vstack([X_real, X_fake])\n",
    "y = np.concatenate([y_real, y_fake])\n",
    "\n",
    "print(\"Final feature matrix:\", X.shape)\n",
    "\n",
    "# -----------------------------\n",
    "# Train/Test Split\n",
    "# -----------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Train XGBoost\n",
    "# -----------------------------\n",
    "clf = XGBClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    tree_method=\"hist\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# -----------------------------\n",
    "# Evaluation\n",
    "# -----------------------------\n",
    "y_pred = clf.predict(X_test)\n",
    "y_proba = clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "print(\"\\n--- Evaluation (XGBoost) ---\")\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_proba))\n",
    "print(\"Accuracy:\", clf.score(X_test, y_test))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# -----------------------------\n",
    "# Save Model\n",
    "# -----------------------------\n",
    "joblib.dump(clf, os.path.join(output_dir, \"xgb_detector.joblib\"))\n",
    "print(\"\\nâœ… Saved XGBoost model to outputs/xgb_detector.joblib\")\n",
    "\n",
    "# -----------------------------\n",
    "# Predict Single Image\n",
    "# -----------------------------\n",
    "def predict_image(image_path, model_path=\"outputs/xgb_detector.joblib\"):\n",
    "    model = joblib.load(model_path)\n",
    "    feat_ela = ela_features(image_path)\n",
    "    feat_fft = fft_features(image_path)\n",
    "    feat_hp  = highpass_features(image_path)\n",
    "    feat_res = deep_features(image_path)\n",
    "    all_feat = np.concatenate([feat_ela, feat_fft, feat_hp, feat_res]).reshape(1,-1)\n",
    "    pred = model.predict(all_feat)[0]\n",
    "    proba = model.predict_proba(all_feat)[0][1]\n",
    "    label = \"FAKE\" if pred == 1 else \"REAL\"\n",
    "    print(f\"Prediction: {label} (prob={proba:.3f})\")\n",
    "    return label, proba\n",
    "\n",
    "# Example usage:\n",
    "# predict_image(r\"C:\\Users\\mujta\\OneDrive\\Desktop\\dataset\\fake\\0001_in.jpg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5d942d-5573-44d6-8fc0-3939faa69018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Running hyperparameter search...\n",
      "Fitting 2 folds for each of 10 candidates, totalling 20 fits\n"
     ]
    }
   ],
   "source": [
    "# ======================================================\n",
    "# Hyperparameter Tuning for XGBoost\n",
    "# ======================================================\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Define parameter search space\n",
    "param_dist = {\n",
    "    \"n_estimators\": [100, 200, 300, 500],\n",
    "    \"max_depth\": [3, 4, 5, 6, 8],\n",
    "    \"learning_rate\": [0.01, 0.05, 0.1, 0.2],\n",
    "    \"subsample\": [0.6, 0.8, 1.0],\n",
    "    \"colsample_bytree\": [0.6, 0.8, 1.0],\n",
    "    \"gamma\": [0, 0.1, 0.2, 0.3]\n",
    "}\n",
    "\n",
    "clf = XGBClassifier(\n",
    "    random_state=42,\n",
    "    tree_method=\"hist\",\n",
    "    n_jobs=-1,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric=\"logloss\"\n",
    ")\n",
    "\n",
    "# RandomizedSearchCV (faster than full grid)\n",
    "random_search = RandomizedSearchCV(\n",
    "    clf,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=10,              # fewer candidates\n",
    "    scoring=\"roc_auc\",\n",
    "    cv=2,                   # fewer CV folds\n",
    "    verbose=2,\n",
    "    random_state=42,\n",
    "    n_jobs=2                # limit parallel jobs\n",
    ")\n",
    "\n",
    "print(\"\\nðŸ” Running hyperparameter search...\")\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Best model\n",
    "best_clf = random_search.best_estimator_\n",
    "print(\"\\nâœ… Best parameters found:\", random_search.best_params_)\n",
    "\n",
    "# Save tuned model\n",
    "joblib.dump(best_clf, os.path.join(output_dir, \"xgb_tuned.joblib\"))\n",
    "print(\"âœ… Saved tuned model to outputs/xgb_tuned.joblib\")\n",
    "\n",
    "# -----------------------------\n",
    "# Evaluate Best Model\n",
    "# -----------------------------\n",
    "y_pred = best_clf.predict(X_test)\n",
    "y_proba = best_clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "print(\"\\n--- Evaluation (Tuned XGBoost) ---\")\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_proba))\n",
    "print(\"Accuracy:\", best_clf.score(X_test, y_test))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
