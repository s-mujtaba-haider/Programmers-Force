{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea86a56d-83dc-4140-84b4-f9ae803b8adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Paste these cells into a Jupyter notebook (or save as .py and run stepwise).\n",
    "# Deepfake detection â€” ViT (RGB) + Custom forensic CNN (ELA, HP, FFT, PRNU-like)\n",
    "# Author: ChatGPT (GPT-5 Thinking mini)\n",
    "# Date: 2025-10-19\n",
    "\n",
    "# -------------------------- CONFIG --------------------------\n",
    "DATA_ROOT = \"./dataset_split\"   # must contain train/real, train/fake, val/real, val/fake, (optional) test/...\n",
    "OUTPUT_DIR = \"./checkpoints\"\n",
    "IMG_SIZE = 224                  # ViT standard: 224 (use 384 if you want higher res and have memory)\n",
    "PATCHES_PER_IMAGE = 4           # random patches per image during training\n",
    "BATCH_SIZE = 16                 # try 16 / 24 / 32 on RTX 3090\n",
    "NUM_WORKERS = 6\n",
    "EPOCHS = 20\n",
    "LR = 1e-4\n",
    "WEIGHT_DECAY = 1e-4\n",
    "MIXUP_ALPHA = 0.2\n",
    "SEED = 42\n",
    "DEVICE = \"cuda\" if __import__(\"torch\").cuda.is_available() else \"cpu\"\n",
    "DEBUG_RUN = False               # set True for a tiny quick sanity run\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# -------------------------- IMPORTS --------------------------\n",
    "import os, random, time, json\n",
    "from glob import glob\n",
    "from io import BytesIO\n",
    "from PIL import Image, ImageChops, ImageFilter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "# timm for ViT\n",
    "try:\n",
    "    import timm\n",
    "    TIMM_AVAILABLE = True\n",
    "except Exception:\n",
    "    TIMM_AVAILABLE = False\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import torch.optim as optim\n",
    "\n",
    "print(\"Device:\", DEVICE, \"timm available:\", TIMM_AVAILABLE)\n",
    "\n",
    "# --------------------- DETERMINISM ---------------------\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available(): torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# ------------------ FORENSIC TRANSFORMS ------------------\n",
    "def compute_ela(pil_img: Image.Image, quality=90) -> Image.Image:\n",
    "    rgb = pil_img.convert('RGB')\n",
    "    buf = BytesIO()\n",
    "    rgb.save(buf, format='JPEG', quality=quality)\n",
    "    buf.seek(0)\n",
    "    comp = Image.open(buf).convert('RGB')\n",
    "    ela = ImageChops.difference(rgb, comp).convert('L')\n",
    "    arr = np.array(ela).astype(np.float32)\n",
    "    maxv = arr.max() if arr.max() > 0 else 1.0\n",
    "    arr = (arr / maxv * 255.0).astype(np.uint8)\n",
    "    return Image.fromarray(arr, mode='L')\n",
    "\n",
    "def high_pass_residual(pil_img: Image.Image, radius=3) -> Image.Image:\n",
    "    rgb = pil_img.convert('RGB')\n",
    "    blurred = rgb.filter(ImageFilter.GaussianBlur(radius=radius))\n",
    "    residual = ImageChops.difference(rgb, blurred).convert('L')\n",
    "    arr = np.array(residual).astype(np.float32)\n",
    "    arr = (arr - arr.min()) / (arr.max()-arr.min()+1e-8) * 255.0\n",
    "    return Image.fromarray(arr.astype(np.uint8), mode='L')\n",
    "\n",
    "def prnu_like_residual(pil_img: Image.Image) -> Image.Image:\n",
    "    rgb = pil_img.convert('RGB')\n",
    "    denoised = rgb.filter(ImageFilter.MedianFilter(size=3))\n",
    "    residual = ImageChops.difference(rgb, denoised).convert('L')\n",
    "    arr = np.array(residual).astype(np.float32)\n",
    "    arr = (arr - arr.min()) / (arr.max()-arr.min()+1e-8) * 255.0\n",
    "    return Image.fromarray(arr.astype(np.uint8), mode='L')\n",
    "\n",
    "def fft_magnitude_map(pil_img: Image.Image) -> Image.Image:\n",
    "    gray = pil_img.convert('L')\n",
    "    arr = np.array(gray).astype(np.float32)\n",
    "    f = np.fft.fft2(arr)\n",
    "    fshift = np.fft.fftshift(f)\n",
    "    mag = np.log(np.abs(fshift) + 1e-8)\n",
    "    mag = (mag - mag.min()) / (mag.max()-mag.min()+1e-8) * 255.0\n",
    "    return Image.fromarray(mag.astype(np.uint8), mode='L')\n",
    "\n",
    "# --------------------- DATASET ---------------------\n",
    "IMG_EXTS = ['.jpg', '.jpeg', '.png', '.bmp']\n",
    "\n",
    "class DeepfakeForensicDataset(Dataset):\n",
    "    def __init__(self, root_dir, split='train', img_size=IMG_SIZE, patches_per_image=PATCHES_PER_IMAGE, mode='train'):\n",
    "        self.root = os.path.join(root_dir, split)\n",
    "        assert os.path.exists(self.root), f\"{self.root} not found\"\n",
    "        self.files = []\n",
    "        for cls, lbl in [('real',0), ('fake',1)]:\n",
    "            folder = os.path.join(self.root, cls)\n",
    "            if not os.path.isdir(folder):\n",
    "                continue\n",
    "            for ext in IMG_EXTS:\n",
    "                self.files += [(p, lbl) for p in glob(os.path.join(folder, f\"*{ext}\"))]\n",
    "        random.shuffle(self.files)\n",
    "        self.img_size = img_size\n",
    "        self.patches = patches_per_image if mode == 'train' else 1\n",
    "        self.mode = mode\n",
    "        self.base_transform = transforms.Compose([transforms.Resize((img_size, img_size)), transforms.ToTensor()])\n",
    "        self.norm = transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "\n",
    "    def __len__(self): return len(self.files)\n",
    "\n",
    "    def read_image(self, path):\n",
    "        return Image.open(path).convert('RGB')\n",
    "\n",
    "    def random_patch_coords(self, w,h,pw,ph):\n",
    "        if w==pw and h==ph: return (0,0,pw,ph)\n",
    "        x = random.randint(0, max(0, w-pw)); y = random.randint(0, max(0, h-ph))\n",
    "        return (x,y,x+pw,y+ph)\n",
    "\n",
    "    def crop_and_compute(self, img, crop_box=None):\n",
    "        crop = img.crop(crop_box) if crop_box else img.copy()\n",
    "        # forensic maps\n",
    "        ela = compute_ela(crop, quality=90).resize((self.img_size,self.img_size))\n",
    "        hp = high_pass_residual(crop, radius=3).resize((self.img_size,self.img_size))\n",
    "        fft = fft_magnitude_map(crop).resize((self.img_size,self.img_size))\n",
    "        pr = prnu_like_residual(crop).resize((self.img_size,self.img_size))\n",
    "        # rgb\n",
    "        rgb = crop.resize((self.img_size,self.img_size))\n",
    "        rgb_t = self.base_transform(rgb)\n",
    "        ela_t = transforms.ToTensor()(ela); hp_t = transforms.ToTensor()(hp)\n",
    "        fft_t = transforms.ToTensor()(fft); pr_t = transforms.ToTensor()(pr)\n",
    "        rgb_t = self.norm(rgb_t)\n",
    "        forensic = torch.cat([ela_t, hp_t, fft_t, pr_t], dim=0)  # [4,H,W]\n",
    "        return rgb_t, forensic\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.files[idx]\n",
    "        img = self.read_image(path)\n",
    "        w,h = img.size\n",
    "        if self.patches == 1:\n",
    "            coords = (0,0,w,h) if (w<=self.img_size and h<=self.img_size) else self.random_patch_coords(w,h,self.img_size,self.img_size)\n",
    "            rgb_t, for_t = self.crop_and_compute(img, crop_box=coords)\n",
    "            return {'rgb': rgb_t, 'forensic': for_t, 'label': torch.tensor(label, dtype=torch.float32), 'path': path}\n",
    "        else:\n",
    "            rgb_patches=[]; for_patches=[]\n",
    "            for _ in range(self.patches):\n",
    "                coords = self.random_patch_coords(w,h,self.img_size,self.img_size)\n",
    "                r,f = self.crop_and_compute(img, crop_box=coords)\n",
    "                rgb_patches.append(r.unsqueeze(0)); for_patches.append(f.unsqueeze(0))\n",
    "            rgb_stack = torch.cat(rgb_patches, dim=0)   # [P,3,H,W]\n",
    "            for_stack = torch.cat(for_patches, dim=0)   # [P,4,H,W]\n",
    "            return {'rgb': rgb_stack, 'forensic': for_stack, 'label': torch.tensor(label, dtype=torch.float32), 'path': path}\n",
    "\n",
    "# --------------------- MODEL ---------------------\n",
    "class ForensicCNN(nn.Module):\n",
    "    def __init__(self, in_ch=4, out_dim=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, 32, 3, padding=1), nn.BatchNorm2d(32), nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d(1)\n",
    "        )\n",
    "        self.out_dim = out_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dim() == 5:\n",
    "            B,P,C,H,W = x.shape\n",
    "            x = x.view(B*P, C, H, W)\n",
    "            o = self.net(x).view(B, P, -1).mean(dim=1)\n",
    "            return o\n",
    "        else:\n",
    "            return self.net(x).view(x.size(0), -1)\n",
    "\n",
    "class ViTForensicsFusion(nn.Module):\n",
    "    def __init__(self, vit_name='vit_base_patch16_224', pretrained=True):\n",
    "        super().__init__()\n",
    "        assert TIMM_AVAILABLE, \"Install timm to use ViT backbones\"\n",
    "        self.vit = timm.create_model(vit_name, pretrained=pretrained, num_classes=0, global_pool='avg')\n",
    "        self.rgb_dim = self.vit.num_features\n",
    "        self.forensic = ForensicCNN(in_ch=4, out_dim=128)\n",
    "        # fusion MLP\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(self.rgb_dim + self.forensic.out_dim, 512),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, rgb, forensic):\n",
    "        # rgb: [B,3,H,W] or [B,P,3,H,W]\n",
    "        if rgb.dim() == 5:\n",
    "            B,P,C,H,W = rgb.shape\n",
    "            rgb = rgb.view(B*P, C, H, W)\n",
    "            rfeat = self.vit(rgb)\n",
    "            rfeat = rfeat.view(B, P, -1).mean(dim=1)\n",
    "        else:\n",
    "            rfeat = self.vit(rgb)\n",
    "        ffeat = self.forensic(forensic)\n",
    "        x = torch.cat([rfeat, ffeat], dim=1)\n",
    "        logits = self.fusion(x).squeeze(1)\n",
    "        return logits\n",
    "\n",
    "# ------------------ TRAIN / EVAL UTILITIES ------------------\n",
    "def mixup_data(x1, x2, y, alpha=0.2):\n",
    "    if alpha <= 0:\n",
    "        return x1, x2, y, None, 1.0\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    batch_size = y.size(0)\n",
    "    index = torch.randperm(batch_size).to(y.device)\n",
    "    x1 = x1 * lam + x1[index] * (1 - lam)\n",
    "    x2 = x2 * lam + x2[index] * (1 - lam)\n",
    "    y_a, y_b = y, y[index]\n",
    "    return x1, x2, y_a, y_b, lam\n",
    "\n",
    "def build_loaders(root):\n",
    "    train_ds = DeepfakeForensicDataset(root, split='train', img_size=IMG_SIZE, patches_per_image=PATCHES_PER_IMAGE, mode='train')\n",
    "    val_ds = DeepfakeForensicDataset(root, split='val', img_size=IMG_SIZE, patches_per_image=1, mode='val')\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    return train_loader, val_loader\n",
    "\n",
    "def train_epoch(model, loader, optimizer, scaler, device, mixup_alpha=0.2):\n",
    "    model.train()\n",
    "    losses=[]; preds=[]; targs=[]\n",
    "    for batch in loader:\n",
    "        rgb = batch['rgb'].to(device); forensic = batch['forensic'].to(device); label = batch['label'].to(device)\n",
    "        if mixup_alpha>0:\n",
    "            rgb, forensic, y_a, y_b, lam = mixup_data(rgb, forensic, label, mixup_alpha)\n",
    "            optimizer.zero_grad()\n",
    "            with autocast():\n",
    "                out = model(rgb, forensic)\n",
    "                loss = lam * F.binary_cross_entropy_with_logits(out, y_a) + (1-lam) * F.binary_cross_entropy_with_logits(out, y_b)\n",
    "            scaler.scale(loss).backward(); scaler.step(optimizer); scaler.update()\n",
    "        else:\n",
    "            optimizer.zero_grad()\n",
    "            with autocast():\n",
    "                out = model(rgb, forensic)\n",
    "                loss = F.binary_cross_entropy_with_logits(out, label)\n",
    "            scaler.scale(loss).backward(); scaler.step(optimizer); scaler.update()\n",
    "        losses.append(loss.item())\n",
    "        probs = torch.sigmoid(out).detach().cpu().numpy().tolist()\n",
    "        preds += probs; targs += label.detach().cpu().numpy().tolist()\n",
    "    auc = roc_auc_score(targs, preds) if len(set(targs))>1 else 0.0\n",
    "    return float(np.mean(losses)), float(auc)\n",
    "\n",
    "def eval_epoch(model, loader, device):\n",
    "    model.eval()\n",
    "    losses=[]; preds=[]; targs=[]\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            rgb = batch['rgb'].to(device); forensic = batch['forensic'].to(device); label = batch['label'].to(device)\n",
    "            out = model(rgb, forensic)\n",
    "            loss = F.binary_cross_entropy_with_logits(out, label)\n",
    "            losses.append(loss.item())\n",
    "            probs = torch.sigmoid(out).detach().cpu().numpy().tolist()\n",
    "            preds += probs; targs += label.detach().cpu().numpy().tolist()\n",
    "    auc = roc_auc_score(targs, preds) if len(set(targs))>1 else 0.0\n",
    "    return float(np.mean(losses)), float(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cdf48f-014f-4204-82d0-903e2b3d16ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------ TRAINING LAUNCH (call cells step-by-step) ------------------\n",
    "if DEBUG_RUN:\n",
    "    # quick dry-run to verify dataloaders & forward pass (VERY small)\n",
    "    print(\"DEBUG_RUN: building loaders and running tiny pass\")\n",
    "    tr_loader, val_loader = build_loaders(DATA_ROOT)\n",
    "    it = iter(tr_loader); batch = next(it)\n",
    "    print(\"Sample batch keys:\", batch.keys())\n",
    "    # prepare model\n",
    "    assert TIMM_AVAILABLE, \"install timm for ViT\"\n",
    "    model = ViTForensicsFusion(vit_name='vit_base_patch16_224', pretrained=True).to(DEVICE)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "    scaler = GradScaler()\n",
    "    # forward pass (may be heavy - if it OOM reduce BATCH_SIZE & IMG_SIZE)\n",
    "    rgb = batch['rgb'].to(DEVICE); forensic = batch['forensic'].to(DEVICE); label=batch['label'].to(DEVICE)\n",
    "    with autocast():\n",
    "        out = model(rgb, forensic)\n",
    "        loss = F.binary_cross_entropy_with_logits(out, label)\n",
    "    print(\"DEBUG forward OK - loss:\", float(loss.detach().cpu().numpy()))\n",
    "else:\n",
    "    print(\"DEBUG_RUN is False. Set DEBUG_RUN = True to perform a quick check run before full training.\")\n",
    "\n",
    "# ------------------ USAGE / TRAINING LOOP (run manually) ------------------\n",
    "# Example training loop you can copy into a cell to run full training:\n",
    "#\n",
    "# train_loader, val_loader = build_loaders(DATA_ROOT)\n",
    "# model = ViTForensicsFusion(vit_name='vit_base_patch16_224', pretrained=True).to(DEVICE)\n",
    "# optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "# scaler = GradScaler()\n",
    "# best_auc = -1.0\n",
    "# for epoch in range(1, EPOCHS+1):\n",
    "#     t0 = time.time()\n",
    "#     tr_loss, tr_auc = train_epoch(model, train_loader, optimizer, scaler, DEVICE, mixup_alpha=MIXUP_ALPHA)\n",
    "#     val_loss, val_auc = eval_epoch(model, val_loader, DEVICE)\n",
    "#     scheduler.step()\n",
    "#     print(f\"Epoch {epoch} | tr_loss {tr_loss:.4f} tr_auc {tr_auc:.4f} | val_loss {val_loss:.4f} val_auc {val_auc:.4f} | time {(time.time()-t0):.1f}s\")\n",
    "#     torch.save({'epoch':epoch,'model_state':model.state_dict(),'optimizer':optimizer.state_dict(),'val_auc':val_auc},\n",
    "#                os.path.join(OUTPUT_DIR, f\"epoch{epoch:02d}_auc{val_auc:.4f}.pth\"))\n",
    "#     if val_auc > best_auc + 1e-4:\n",
    "#         best_auc = val_auc\n",
    "#         torch.save({'epoch':epoch,'model_state':model.state_dict(),'optimizer':optimizer.state_dict(),'val_auc':val_auc},\n",
    "#                    os.path.join(OUTPUT_DIR, \"best.pth\"))\n",
    "#     # optional early stopping check here\n",
    "#\n",
    "# After training: load best.pth and use inference helper shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f7cb48-e6c9-4c35-87f5-203668e1fc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ------------------ INFERENCE HELPER ------------------\n",
    "def predict_image(model, path, device, top_k_patches=PATCHES_PER_IMAGE):\n",
    "    model.eval()\n",
    "    img = Image.open(path).convert('RGB')\n",
    "    w,h = img.size\n",
    "    patches = []\n",
    "    for i in range(top_k_patches):\n",
    "        x = random.randint(0, max(0, w-IMG_SIZE))\n",
    "        y = random.randint(0, max(0, h-IMG_SIZE))\n",
    "        crop = img.crop((x,y,x+IMG_SIZE,y+IMG_SIZE)).resize((IMG_SIZE,IMG_SIZE))\n",
    "        rgb_t = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=[0.485,0.456,0.406],std=[0.229,0.224,0.225])])(crop).unsqueeze(0)\n",
    "        ela = transforms.ToTensor()(compute_ela(crop)).unsqueeze(0)\n",
    "        hp = transforms.ToTensor()(high_pass_residual(crop)).unsqueeze(0)\n",
    "        fft = transforms.ToTensor()(fft_magnitude_map(crop)).unsqueeze(0)\n",
    "        pr = transforms.ToTensor()(prnu_like_residual(crop)).unsqueeze(0)\n",
    "        forensic = torch.cat([ela,hp,fft,pr], dim=1)\n",
    "        patches.append((rgb_t, forensic))\n",
    "    rgbs = torch.cat([p[0] for p in patches], dim=0).unsqueeze(0)  # [1,P,3,H,W]\n",
    "    fors = torch.cat([p[1] for p in patches], dim=0).unsqueeze(0)   # [1,P,4,H,W]\n",
    "    with torch.no_grad():\n",
    "        logits = model(rgbs.to(device), fors.to(device))\n",
    "        probs = torch.sigmoid(logits).cpu().numpy().tolist()\n",
    "    return float(np.mean(probs)), probs\n",
    "\n",
    "# ------------------ QUICK VISUALIZATION OF A SAMPLE (optional) ------------------\n",
    "def visualize_sample(path):\n",
    "    img = Image.open(path).convert('RGB')\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.subplot(1,2,1); plt.imshow(img); plt.axis('off'); plt.title('RGB')\n",
    "    f = fft_magnitude_map(img)\n",
    "    plt.subplot(1,2,2); plt.imshow(f, cmap='gray'); plt.axis('off'); plt.title('FFT magnitude')\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
