{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb64f4e6-cea4-4f60-845c-f37c883cd29b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Path to FF++ dataset: C:\\Users\\mujta\\.cache\\kagglehub\\datasets\\xdxd003\\ff-c23\\versions\\1\n",
      "1/\n",
      "    FaceForensics++_C23/\n",
      "        csv/\n",
      "            DeepFakeDetection.csv\n",
      "            Deepfakes.csv\n",
      "            Face2Face.csv\n",
      "            FaceShifter.csv\n",
      "            FaceSwap.csv\n",
      "            FF++_Metadata.csv\n",
      "            FF++_Metadata_Shuffled.csv\n",
      "            Mean_Data.csv\n",
      "            NeuralTextures.csv\n",
      "            original.csv\n",
      "        DeepFakeDetection/\n",
      "            01_02__meeting_serious__YVGY8LOK.mp4\n",
      "            01_02__outside_talking_still_laughing__YVGY8LOK.mp4\n",
      "            01_02__talking_against_wall__YVGY8LOK.mp4\n",
      "            01_02__walking_down_indoor_hall_disgust__YVGY8LOK.mp4\n",
      "            01_02__walk_down_hall_angry__YVGY8LOK.mp4\n",
      "            01_03__hugging_happy__ISF9SP4G.mp4\n",
      "            01_03__kitchen_pan__JZUXXFRB.mp4\n",
      "            01_03__podium_speech_happy__480LQD1C.mp4\n",
      "            01_03__talking_against_wall__JZUXXFRB.mp4\n",
      "            01_04__hugging_happy__GBC7ZGDP.mp4\n",
      "            ... (1000 files total)\n",
      "        Deepfakes/\n",
      "            000_003.mp4\n",
      "            001_870.mp4\n",
      "            002_006.mp4\n",
      "            003_000.mp4\n",
      "            004_982.mp4\n",
      "            005_010.mp4\n",
      "            006_002.mp4\n",
      "            007_132.mp4\n",
      "            008_990.mp4\n",
      "            009_027.mp4\n",
      "            ... (1000 files total)\n",
      "        Face2Face/\n",
      "            000_003.mp4\n",
      "            001_870.mp4\n",
      "            002_006.mp4\n",
      "            003_000.mp4\n",
      "            004_982.mp4\n",
      "            005_010.mp4\n",
      "            006_002.mp4\n",
      "            007_132.mp4\n",
      "            008_990.mp4\n",
      "            009_027.mp4\n",
      "            ... (1000 files total)\n",
      "        FaceShifter/\n",
      "            000_003.mp4\n",
      "            001_870.mp4\n",
      "            002_006.mp4\n",
      "            003_000.mp4\n",
      "            004_982.mp4\n",
      "            005_010.mp4\n",
      "            006_002.mp4\n",
      "            007_132.mp4\n",
      "            008_990.mp4\n",
      "            009_027.mp4\n",
      "            ... (1000 files total)\n",
      "        FaceSwap/\n",
      "            000_003.mp4\n",
      "            001_870.mp4\n",
      "            002_006.mp4\n",
      "            003_000.mp4\n",
      "            004_982.mp4\n",
      "            005_010.mp4\n",
      "            006_002.mp4\n",
      "            007_132.mp4\n",
      "            008_990.mp4\n",
      "            009_027.mp4\n",
      "            ... (1000 files total)\n",
      "        NeuralTextures/\n",
      "            000_003.mp4\n",
      "            001_870.mp4\n",
      "            002_006.mp4\n",
      "            003_000.mp4\n",
      "            004_982.mp4\n",
      "            005_010.mp4\n",
      "            006_002.mp4\n",
      "            007_132.mp4\n",
      "            008_990.mp4\n",
      "            009_027.mp4\n",
      "            ... (1000 files total)\n",
      "        original/\n",
      "            000.mp4\n",
      "            001.mp4\n",
      "            002.mp4\n",
      "            003.mp4\n",
      "            004.mp4\n",
      "            005.mp4\n",
      "            006.mp4\n",
      "            007.mp4\n",
      "            008.mp4\n",
      "            009.mp4\n",
      "            ... (1000 files total)\n",
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/minhnh2107/casiafasd?dataset_version_number=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 70.6M/70.6M [00:07<00:00, 9.98MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Path to CASIA-FASD dataset: C:\\Users\\mujta\\.cache\\kagglehub\\datasets\\minhnh2107\\casiafasd\\versions\\1\n",
      "1/\n",
      "    test_img/\n",
      "        test_img/\n",
      "            color/\n",
      "                10_1.avi_100_real.jpg\n",
      "                10_1.avi_125_real.jpg\n",
      "                10_1.avi_25_real.jpg\n",
      "                10_1.avi_50_real.jpg\n",
      "                10_1.avi_75_real.jpg\n",
      "                10_2.avi_100_real.jpg\n",
      "                10_2.avi_125_real.jpg\n",
      "                10_2.avi_150_real.jpg\n",
      "                10_2.avi_175_real.jpg\n",
      "                10_2.avi_25_real.jpg\n",
      "                ... (2408 files total)\n",
      "            depth/\n",
      "                10_1.avi_100_real.jpg\n",
      "                10_1.avi_125_real.jpg\n",
      "                10_1.avi_25_real.jpg\n",
      "                10_1.avi_50_real.jpg\n",
      "                10_1.avi_75_real.jpg\n",
      "                10_2.avi_100_real.jpg\n",
      "                10_2.avi_125_real.jpg\n",
      "                10_2.avi_150_real.jpg\n",
      "                10_2.avi_175_real.jpg\n",
      "                10_2.avi_25_real.jpg\n",
      "                ... (2408 files total)\n",
      "    train_img/\n",
      "        train_img/\n",
      "            color/\n",
      "                10_1.avi_100_real.jpg\n",
      "                10_1.avi_25_real.jpg\n",
      "                10_1.avi_50_real.jpg\n",
      "                10_1.avi_75_real.jpg\n",
      "                10_2.avi_100_real.jpg\n",
      "                10_2.avi_125_real.jpg\n",
      "                10_2.avi_25_real.jpg\n",
      "                10_2.avi_50_real.jpg\n",
      "                10_2.avi_75_real.jpg\n",
      "                10_3.avi_25_fake.jpg\n",
      "                ... (1655 files total)\n",
      "            depth/\n",
      "                10_1.avi_100_real.jpg\n",
      "                10_1.avi_25_real.jpg\n",
      "                10_1.avi_50_real.jpg\n",
      "                10_1.avi_75_real.jpg\n",
      "                10_2.avi_100_real.jpg\n",
      "                10_2.avi_125_real.jpg\n",
      "                10_2.avi_25_real.jpg\n",
      "                10_2.avi_50_real.jpg\n",
      "                10_2.avi_75_real.jpg\n",
      "                10_3.avi_25_fake.jpg\n",
      "                ... (1655 files total)\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "import os\n",
    "\n",
    "# =============================\n",
    "# Download FaceForensics++ (FF++)\n",
    "# =============================\n",
    "ffpp_path = kagglehub.dataset_download(\"xdxd003/ff-c23\")\n",
    "print(\"✅ Path to FF++ dataset:\", ffpp_path)\n",
    "\n",
    "# Show the folder structure of FF++\n",
    "for root, dirs, files in os.walk(ffpp_path):\n",
    "    level = root.replace(ffpp_path, \"\").count(os.sep)\n",
    "    indent = \" \" * 4 * (level)\n",
    "    print(f\"{indent}{os.path.basename(root)}/\")\n",
    "    subindent = \" \" * 4 * (level + 1)\n",
    "    for f in files[:10]:  # show only first 10 files per directory\n",
    "        print(f\"{subindent}{f}\")\n",
    "    if len(files) > 10:\n",
    "        print(f\"{subindent}... ({len(files)} files total)\")\n",
    "\n",
    "# =============================\n",
    "# Download CASIA-FASD\n",
    "# =============================\n",
    "casia_path = kagglehub.dataset_download(\"minhnh2107/casiafasd\")\n",
    "print(\"\\n✅ Path to CASIA-FASD dataset:\", casia_path)\n",
    "\n",
    "# Show the folder structure of CASIA-FASD\n",
    "for root, dirs, files in os.walk(casia_path):\n",
    "    level = root.replace(casia_path, \"\").count(os.sep)\n",
    "    indent = \" \" * 4 * (level)\n",
    "    print(f\"{indent}{os.path.basename(root)}/\")\n",
    "    subindent = \" \" * 4 * (level + 1)\n",
    "    for f in files[:10]:  # show only first 10 files per directory\n",
    "        print(f\"{subindent}{f}\")\n",
    "    if len(files) > 10:\n",
    "        print(f\"{subindent}... ({len(files)} files total)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9464fda5-eb72-41c5-90b2-a3f7beec9616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting FF++ REAL...\n",
      "Extracting FF++ DEEPFAKE...\n",
      "Copying CASIA-FASD (as REAL)...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms, datasets\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "# ======================\n",
    "# CONFIG\n",
    "# ======================\n",
    "ffpp_path = r\"C:\\Users\\mujta\\.cache\\kagglehub\\datasets\\xdxd003\\ff-c23\\versions\\1\\FaceForensics++_C23\"\n",
    "casia_path = r\"C:\\Users\\mujta\\.cache\\kagglehub\\datasets\\minhnh2107\\casiafasd\\versions\\1\"\n",
    "\n",
    "output_data = \"processed_dataset\"  # final dataset folder\n",
    "os.makedirs(output_data, exist_ok=True)\n",
    "\n",
    "# Target size for Inception\n",
    "IMG_SIZE = 299\n",
    "\n",
    "# ======================\n",
    "# HELPER: Extract frames from videos\n",
    "# ======================\n",
    "def extract_frames(video_path, save_dir, label, max_frames=10):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_indices = sorted(random.sample(range(total_frames), min(max_frames, total_frames)))\n",
    "\n",
    "    for i, idx in enumerate(frame_indices):\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            img = Image.fromarray(frame)\n",
    "            img = img.resize((IMG_SIZE, IMG_SIZE))\n",
    "            img.save(os.path.join(save_dir, f\"{label}_{i}.jpg\"))\n",
    "    cap.release()\n",
    "\n",
    "# ======================\n",
    "# PREPARE DATASET\n",
    "# ======================\n",
    "\n",
    "# FF++ REAL (original videos)\n",
    "real_out = os.path.join(output_data, \"real\")\n",
    "deepfake_out = os.path.join(output_data, \"deepfake\")\n",
    "os.makedirs(real_out, exist_ok=True)\n",
    "os.makedirs(deepfake_out, exist_ok=True)\n",
    "\n",
    "print(\"Extracting FF++ REAL...\")\n",
    "for f in os.listdir(os.path.join(ffpp_path, \"original\"))[:50]:  # limit for testing\n",
    "    video_path = os.path.join(ffpp_path, \"original\", f)\n",
    "    extract_frames(video_path, real_out, \"real\")\n",
    "\n",
    "print(\"Extracting FF++ DEEPFAKE...\")\n",
    "for folder in [\"Deepfakes\", \"Face2Face\", \"FaceSwap\", \"FaceShifter\", \"NeuralTextures\"]:\n",
    "    for f in os.listdir(os.path.join(ffpp_path, folder))[:50]:\n",
    "        video_path = os.path.join(ffpp_path, folder, f)\n",
    "        extract_frames(video_path, deepfake_out, \"deepfake\")\n",
    "\n",
    "print(\"Copying CASIA-FASD (as REAL)...\")\n",
    "casia_train = os.path.join(casia_path, \"train_img\", \"train_img\", \"color\")\n",
    "for f in os.listdir(casia_train)[:500]:\n",
    "    img = Image.open(os.path.join(casia_train, f)).convert(\"RGB\").resize((IMG_SIZE, IMG_SIZE))\n",
    "    img.save(os.path.join(real_out, f\"casia_{f}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48410838-fd6a-42cc-b980-36aac7cd1fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mujta\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mujta\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/inception_v3_google-0cc3c7bd.pth\" to C:\\Users\\mujta/.cache\\torch\\hub\\checkpoints\\inception_v3_google-0cc3c7bd.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 104M/104M [00:09<00:00, 11.2MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5] Loss: 0.2303, Acc: 92.88%\n",
      "Epoch [2/5] Loss: 0.0416, Acc: 98.08%\n",
      "Epoch [3/5] Loss: 0.0338, Acc: 98.27%\n",
      "Epoch [4/5] Loss: 0.0322, Acc: 98.46%\n",
      "Epoch [5/5] Loss: 0.0291, Acc: 99.23%\n"
     ]
    }
   ],
   "source": [
    "# ======================\n",
    "# DATALOADER\n",
    "# ======================\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5,0.5,0.5], [0.5,0.5,0.5])\n",
    "])\n",
    "\n",
    "dataset = datasets.ImageFolder(output_data, transform=transform)\n",
    "train_loader = DataLoader(dataset, batch_size=16, shuffle=True, num_workers=2)\n",
    "\n",
    "# ======================\n",
    "# MODEL\n",
    "# ======================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = models.inception_v3(pretrained=True)\n",
    "model.aux_logits = False\n",
    "model.fc = nn.Linear(model.fc.in_features, 2)  # binary classification\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# ======================\n",
    "# TRAIN\n",
    "# ======================\n",
    "EPOCHS = 5\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "    for imgs, labels in train_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}] Loss: {running_loss/len(train_loader):.4f}, Acc: {100.*correct/total:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "28b28466-95ac-4182-921a-2b33c7fa14c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REAL\n"
     ]
    }
   ],
   "source": [
    "def predict_face_image(img_path):\n",
    "    model.eval()\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    face = mtcnn(img)\n",
    "    if face is None:\n",
    "        return \"No face detected\"\n",
    "    \n",
    "    # Ensure consistency: MTCNN → PIL → training transform\n",
    "    face_img = transforms.ToPILImage()(face)\n",
    "    face = transform(face_img).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(face)\n",
    "        pred = torch.argmax(outputs, dim=1).item()\n",
    "    return \"DEEPFAKE\" if pred == 0 else \"REAL\"\n",
    "\n",
    "\n",
    "print(predict_face_image(r\"C:\\Users\\mujta\\OneDrive\\Desktop\\FaceSwap_10.jpg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9587df04-4320-4e18-8bc7-b11133b2aaf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REAL\n",
      "DEEPFAKE\n",
      "REAL\n"
     ]
    }
   ],
   "source": [
    "print(predict_image(os.path.join(real_out, os.listdir(real_out)[0])))\n",
    "print(predict_image(os.path.join(deepfake_out, os.listdir(deepfake_out)[0])))\n",
    "\n",
    "print(predict_image(r\"C:\\Users\\mujta\\OneDrive\\Desktop\\DALL-E_0004.jpg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86324c4b-07de-4d51-b6c1-8400bc92f005",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_own_image(img_path):\n",
    "    model.eval()\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    img = transform(img).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(img)\n",
    "        pred = torch.argmax(outputs, dim=1).item()\n",
    "    return \"DEEPFAKE\" if pred == 0 else \"REAL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c60f9f0-cefb-48c4-b196-81cf8aa24462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: REAL\n"
     ]
    }
   ],
   "source": [
    "my_image = r\"C:\\Users\\mujta\\OneDrive\\Desktop\\DALL-E_0004.jpg\"  # <-- replace with your file\n",
    "print(\"Prediction:\", predict_own_image(my_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d339e8-c4f9-4df8-aa1b-03acfbb9599a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a sample deepfake video from FF++\n",
    "test_video = r\"C:\\Users\\mujta\\OneDrive\\Desktop\\me.jpg\"\n",
    "\n",
    "# Extract 1 frame from it\n",
    "cap = cv2.VideoCapture(test_video)\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES, 50)  # pick frame number 50\n",
    "ret, frame = cap.read()\n",
    "cap.release()\n",
    "\n",
    "if ret:\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    test_img = Image.fromarray(frame).resize((IMG_SIZE, IMG_SIZE))\n",
    "    test_img.save(\"deepfake_test.jpg\")\n",
    "\n",
    "print(\"Prediction:\", predict_image(\"deepfake_test.jpg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e7c8a730-6ac1-45b1-bbd2-d88d630f4b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 416, Test size: 104\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "# total size of dataset\n",
    "total_size = len(dataset)\n",
    "train_size = int(0.8 * total_size)\n",
    "test_size = total_size - train_size\n",
    "\n",
    "# split dataset\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"Train size: {len(train_dataset)}, Test size: {len(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ab1c6e20-a977-4b22-ad16-8772316c8cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 99.04%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct, total = 0, 0\n",
    "all_preds, all_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in test_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        outputs = model(imgs)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "acc = 100. * correct / total\n",
    "print(f\"Test Accuracy: {acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7ae28325-e37d-4de5-b896-657a4755430a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CASIA-FASD downloaded to: C:\\Users\\mujta\\.cache\\kagglehub\\datasets\\minhnh2107\\casiafasd\\versions\\1\n",
      "1/\n",
      "    test_img/\n",
      "        test_img/\n",
      "            color/\n",
      "                10_1.avi_100_real.jpg\n",
      "                10_1.avi_125_real.jpg\n",
      "                10_1.avi_25_real.jpg\n",
      "                10_1.avi_50_real.jpg\n",
      "                10_1.avi_75_real.jpg\n",
      "            depth/\n",
      "                10_1.avi_100_real.jpg\n",
      "                10_1.avi_125_real.jpg\n",
      "                10_1.avi_25_real.jpg\n",
      "                10_1.avi_50_real.jpg\n",
      "                10_1.avi_75_real.jpg\n",
      "    train_img/\n",
      "        train_img/\n",
      "            color/\n",
      "                10_1.avi_100_real.jpg\n",
      "                10_1.avi_25_real.jpg\n",
      "                10_1.avi_50_real.jpg\n",
      "                10_1.avi_75_real.jpg\n",
      "                10_2.avi_100_real.jpg\n",
      "            depth/\n",
      "                10_1.avi_100_real.jpg\n",
      "                10_1.avi_25_real.jpg\n",
      "                10_1.avi_50_real.jpg\n",
      "                10_1.avi_75_real.jpg\n",
      "                10_2.avi_100_real.jpg\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download CASIA-FASD dataset\n",
    "casia_path = kagglehub.dataset_download(\"minhnh2107/casiafasd\")\n",
    "\n",
    "print(\"✅ CASIA-FASD downloaded to:\", casia_path)\n",
    "\n",
    "# Check structure\n",
    "import os\n",
    "\n",
    "for root, dirs, files in os.walk(casia_path):\n",
    "    level = root.replace(casia_path, '').count(os.sep)\n",
    "    indent = ' ' * 4 * level\n",
    "    print(f\"{indent}{os.path.basename(root)}/\")\n",
    "    subindent = ' ' * 4 * (level + 1)\n",
    "    for f in files[:5]:  # show first 5 files per folder\n",
    "        print(f\"{subindent}{f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "af9c4e55-6eee-4b78-a9dc-a737815c152a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CASIA reorganized into: C:\\Users\\mujta\\.cache\\kagglehub\\datasets\\minhnh2107\\casiafasd\\versions\\1\\processed_casia\n"
     ]
    }
   ],
   "source": [
    "import os, shutil\n",
    "\n",
    "casia_base = r\"C:\\Users\\mujta\\.cache\\kagglehub\\datasets\\minhnh2107\\casiafasd\\versions\\1\"\n",
    "output_path = os.path.join(casia_base, \"processed_casia\")\n",
    "\n",
    "# source paths\n",
    "train_src = os.path.join(casia_base, \"train_img\", \"train_img\", \"color\")\n",
    "test_src  = os.path.join(casia_base, \"test_img\", \"test_img\", \"color\")\n",
    "\n",
    "for split, src in [(\"train\", train_src), (\"test\", test_src)]:\n",
    "    real_dir = os.path.join(output_path, split, \"real\")\n",
    "    fake_dir = os.path.join(output_path, split, \"fake\")\n",
    "    os.makedirs(real_dir, exist_ok=True)\n",
    "    os.makedirs(fake_dir, exist_ok=True)\n",
    "    \n",
    "    for file in os.listdir(src):\n",
    "        if file.endswith(\".jpg\"):\n",
    "            src_file = os.path.join(src, file)\n",
    "            if \"real\" in file.lower():\n",
    "                shutil.copy(src_file, os.path.join(real_dir, file))\n",
    "            elif \"fake\" in file.lower():\n",
    "                shutil.copy(src_file, os.path.join(fake_dir, file))\n",
    "\n",
    "print(\"✅ CASIA reorganized into:\", output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2ad2a241-46e3-4830-a1d2-40e5868f88b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['fake', 'real']\n",
      "Total Test Images: 2408\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "IMG_SIZE = 299\n",
    "\n",
    "casia_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5,0.5,0.5], [0.5,0.5,0.5])\n",
    "])\n",
    "\n",
    "casia_test_dataset = datasets.ImageFolder(\n",
    "    os.path.join(output_path, \"test\"),\n",
    "    transform=casia_transform\n",
    ")\n",
    "\n",
    "casia_test_loader = DataLoader(casia_test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "print(\"Classes:\", casia_test_dataset.classes)   # should print ['fake', 'real']\n",
    "print(\"Total Test Images:\", len(casia_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "764bf685-5e94-48c1-bdef-00559aab1d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CASIA Test Accuracy: 24.54%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fake       0.00      0.00      0.00      1817\n",
      "        real       0.25      1.00      0.39       591\n",
      "\n",
      "    accuracy                           0.25      2408\n",
      "   macro avg       0.12      0.50      0.20      2408\n",
      "weighted avg       0.06      0.25      0.10      2408\n",
      "\n",
      "Confusion Matrix:\n",
      "[[   0 1817]\n",
      " [   0  591]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mujta\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "C:\\Users\\mujta\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "C:\\Users\\mujta\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# make sure model is in eval mode\n",
    "model.eval()\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in casia_test_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        outputs = model(imgs)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        \n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Metrics\n",
    "acc = accuracy_score(all_labels, all_preds)\n",
    "print(f\"✅ CASIA Test Accuracy: {acc*100:.2f}%\\n\")\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=casia_test_dataset.classes))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(all_labels, all_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcde0d08-9c67-451c-8645-a71cac2a3376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting FF++ originals (REAL)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|█████████████████████████████▋                                               | 385/1000 [46:14<2:02:03, 11.91s/it]"
     ]
    }
   ],
   "source": [
    "# Cell 1 -- Setup + extract frames (FF++) + reorganize CASIA if not already done\n",
    "import os, random, shutil, math\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === CONFIG ===\n",
    "ffpp_path = r\"C:\\Users\\mujta\\.cache\\kagglehub\\datasets\\xdxd003\\ff-c23\\versions\\1\\FaceForensics++_C23\"\n",
    "casia_base = r\"C:\\Users\\mujta\\.cache\\kagglehub\\datasets\\minhnh2107\\casiafasd\\versions\\1\"\n",
    "output_data = r\"C:\\Users\\mujta\\processed_dataset_v2\"   # change if you want\n",
    "os.makedirs(output_data, exist_ok=True)\n",
    "\n",
    "IMG_SIZE = 299\n",
    "FRAMES_PER_VIDEO = 50      # increase to extract more frames (set <= total frames)\n",
    "MAX_VIDEOS_PER_FOLDER = None  # None = use all, or set an integer for debugging\n",
    "\n",
    "# Helper: evenly spaced frame indices\n",
    "def pick_frame_indices(total_frames, k):\n",
    "    if total_frames <= 0:\n",
    "        return []\n",
    "    k = min(k, total_frames)\n",
    "    step = total_frames / k\n",
    "    return [int(i * step) for i in range(k)]\n",
    "\n",
    "# Extract frames (face cropping will be applied later via MTCNN during dataset)\n",
    "def extract_frames_from_video(video_path, save_dir, prefix, frames_to_extract=50):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) or 0\n",
    "    indices = pick_frame_indices(total_frames, frames_to_extract)\n",
    "    saved = 0\n",
    "    for i, idx in enumerate(indices):\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            continue\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        img = Image.fromarray(frame).resize((IMG_SIZE, IMG_SIZE))  # temporary resize; final face crop uses MTCNN\n",
    "        fname = f\"{prefix}_{Path(video_path).stem}_{i:03d}.jpg\"\n",
    "        img.save(os.path.join(save_dir, fname))\n",
    "        saved += 1\n",
    "    cap.release()\n",
    "    return saved\n",
    "\n",
    "# Prepare folders\n",
    "real_out = os.path.join(output_data, \"train\", \"real\")\n",
    "fake_out = os.path.join(output_data, \"train\", \"fake\")\n",
    "os.makedirs(real_out, exist_ok=True)\n",
    "os.makedirs(fake_out, exist_ok=True)\n",
    "\n",
    "# 1) Extract from FF++ originals -> REAL\n",
    "print(\"Extracting FF++ originals (REAL)...\")\n",
    "orig_dir = os.path.join(ffpp_path, \"original\")\n",
    "orig_videos = sorted(os.listdir(orig_dir))\n",
    "if MAX_VIDEOS_PER_FOLDER:\n",
    "    orig_videos = orig_videos[:MAX_VIDEOS_PER_FOLDER]\n",
    "for v in tqdm(orig_videos):\n",
    "    p = os.path.join(orig_dir, v)\n",
    "    extract_frames_from_video(p, real_out, \"ffpp_orig\", FRAMES_PER_VIDEO)\n",
    "\n",
    "# 2) Extract from FF++ manipulated -> FAKE\n",
    "manip_folders = [\"Deepfakes\", \"Face2Face\", \"FaceSwap\", \"FaceShifter\", \"NeuralTextures\"]\n",
    "print(\"Extracting FF++ manipulated (DEEPFAKE)...\")\n",
    "for folder in manip_folders:\n",
    "    folder_path = os.path.join(ffpp_path, folder)\n",
    "    vids = sorted(os.listdir(folder_path))\n",
    "    if MAX_VIDEOS_PER_FOLDER:\n",
    "        vids = vids[:MAX_VIDEOS_PER_FOLDER]\n",
    "    for v in tqdm(vids, desc=folder):\n",
    "        p = os.path.join(folder_path, v)\n",
    "        extract_frames_from_video(p, fake_out, f\"ffpp_{folder}\", FRAMES_PER_VIDEO)\n",
    "\n",
    "# 3) Add CASIA images as REAL (use both train/test color sets)\n",
    "print(\"Copying CASIA images (as REAL)...\")\n",
    "casia_color_train = os.path.join(casia_base, \"train_img\", \"train_img\", \"color\")\n",
    "casia_color_test  = os.path.join(casia_base, \"test_img\", \"test_img\", \"color\")\n",
    "for src in [casia_color_train, casia_color_test]:\n",
    "    if not os.path.isdir(src):\n",
    "        continue\n",
    "    for f in os.listdir(src):\n",
    "        if not f.lower().endswith(\".jpg\"):\n",
    "            continue\n",
    "        # treat both 'real' and 'fake' as REAL? No — in your spec earlier, CASIA spoofed reals should be real.\n",
    "        # But CASIA contains *_fake.jpg naming (these are replay/print attacks) — earlier you wanted spoofed real to be REAL.\n",
    "        # So treat *_fake.jpg from CASIA as REAL (presentation attacks = real). If you want them as fake, change logic.\n",
    "        src_fp = os.path.join(src, f)\n",
    "        # If filename indicates \"real\" -> copy to real; if \"fake\" -> also copy to real (per your requirement)\n",
    "        target = real_out\n",
    "        shutil.copy(src_fp, os.path.join(target, f\"casia_{f}\"))\n",
    "\n",
    "print(\"Done extraction and CASIA copy. Train real images:\", len(os.listdir(real_out)), \"Train fake images:\", len(os.listdir(fake_out)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc761ae-b88c-4840-8c42-424bb658e9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2 -- dataset + transforms + dataloaders\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, WeightedRandomSampler\n",
    "from torchvision import transforms\n",
    "from facenet_pytorch import MTCNN\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# MTCNN face detector; returns face tensors (C,H,W) in range 0-1\n",
    "mtcnn = MTCNN(image_size=IMG_SIZE, margin=20, keep_all=False, device=device)\n",
    "\n",
    "# Transforms: training with augmentation, validation/test only resize+norm\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(0.1, 0.1, 0.1, 0.05),\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5,0.5,0.5], [0.5,0.5,0.5])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5,0.5,0.5], [0.5,0.5,0.5])\n",
    "])\n",
    "\n",
    "class FaceFolderDataset(Dataset):\n",
    "    \"\"\"Loads images from folder structure root/real and root/fake.\n",
    "       Uses MTCNN to crop the face. If no face found, falls back to center-crop.\n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir, transform=None, mtcnn=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.mtcnn = mtcnn\n",
    "        self.samples = []\n",
    "        for label_name, label_idx in [(\"real\", 0), (\"fake\", 1)]:  # choose mapping: 0=real,1=fake\n",
    "            folder = os.path.join(root_dir, label_name)\n",
    "            if not os.path.isdir(folder):\n",
    "                continue\n",
    "            for f in os.listdir(folder):\n",
    "                if f.lower().endswith((\".jpg\",\".jpeg\",\".png\")):\n",
    "                    self.samples.append((os.path.join(folder, f), label_idx))\n",
    "        # shuffle stable\n",
    "        random.shuffle(self.samples)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.samples[idx]\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        face = None\n",
    "        if self.mtcnn is not None:\n",
    "            # mtcnn returns tensor or None\n",
    "            try:\n",
    "                face = self.mtcnn(img)  # tensor CxHxW in 0..1\n",
    "            except Exception as e:\n",
    "                face = None\n",
    "        if face is not None:\n",
    "            # convert back to PIL so same transforms apply\n",
    "            face_img = transforms.ToPILImage()(face)\n",
    "        else:\n",
    "            # fallback: center crop from original\n",
    "            w, h = img.size\n",
    "            min_side = min(w,h)\n",
    "            left = (w-min_side)//2\n",
    "            top = (h-min_side)//2\n",
    "            face_img = img.crop((left, top, left+min_side, top+min_side))\n",
    "        if self.transform:\n",
    "            face_img = self.transform(face_img)\n",
    "        return face_img, label\n",
    "\n",
    "# Create datasets\n",
    "train_root = os.path.join(output_data, \"train\")\n",
    "os.makedirs(train_root, exist_ok=True)  # should exist\n",
    "\n",
    "# Create dataset, then split into train/val\n",
    "full_dataset = FaceFolderDataset(train_root, transform=train_transform, mtcnn=mtcnn)\n",
    "val_ratio = 0.15\n",
    "val_size = int(len(full_dataset) * val_ratio)\n",
    "train_size = len(full_dataset) - val_size\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "# For val dataset, use val_transform (we need to replace transform in subset)\n",
    "# hack: override transform for items in val_dataset\n",
    "val_dataset.dataset.transform = val_transform\n",
    "\n",
    "# Weighted sampler to handle class imbalance\n",
    "labels = [label for _, label in full_dataset.samples]\n",
    "class_sample_count = np.array([labels.count(0), labels.count(1)])  # [real_count, fake_count]\n",
    "print(\"Counts (real,fake):\", class_sample_count)\n",
    "weight = 1. / class_sample_count\n",
    "samples_weight = np.array([weight[t] for t in labels])\n",
    "sampler = WeightedRandomSampler(weights=samples_weight, num_samples=len(samples_weight), replacement=True)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler, num_workers=4, pin_memory=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "print(\"Train size:\", len(train_dataset), \"Val size:\", len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5512e438-9f52-4e5c-be68-3f1c4fc8aedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3 -- model build, optimizer, criterion, scheduler\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "\n",
    "# Load pretrained Inception v3 (use weights interface if torchvision version supports it)\n",
    "model = models.inception_v3(weights=models.Inception_V3_Weights.IMAGENET1K_V1) if hasattr(models, \"Inception_V3_Weights\") else models.inception_v3(pretrained=True)\n",
    "model.aux_logits = False\n",
    "\n",
    "# Replace fc with dropout + classifier\n",
    "in_features = model.fc.in_features\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(in_features, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.4),\n",
    "    nn.Linear(512, 2)   # 2 classes: 0=REAL, 1=DEEPFAKE\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Option: freeze early layers for first epochs (uncomment if you want)\n",
    "# for name, param in model.named_parameters():\n",
    "#     if \"fc\" not in name:\n",
    "#         param.requires_grad = False\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c433b092-1ba2-4b54-ad14-f3da4eb7eb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4 -- training loop\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "EPOCHS = 10\n",
    "best_val_loss = float(\"inf\")\n",
    "best_model_path = os.path.join(output_data, \"best_inception_v3.pth\")\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    t0 = time.time()\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    for imgs, labels in train_loader:\n",
    "        imgs = imgs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "        preds = outputs.argmax(dim=1).detach().cpu().numpy()\n",
    "        all_preds.extend(list(preds))\n",
    "        all_labels.extend(list(labels.cpu().numpy()))\n",
    "\n",
    "    train_loss = sum(train_losses)/len(train_losses)\n",
    "    train_acc = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    val_preds = []\n",
    "    val_labels = []\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in val_loader:\n",
    "            imgs = imgs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_losses.append(loss.item())\n",
    "            preds = outputs.argmax(dim=1).cpu().numpy()\n",
    "            val_preds.extend(list(preds))\n",
    "            val_labels.extend(list(labels.cpu().numpy()))\n",
    "\n",
    "    val_loss = sum(val_losses)/len(val_losses)\n",
    "    val_acc = accuracy_score(val_labels, val_preds)\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch}/{EPOCHS}  Time: {time.time()-t0:.1f}s  TrainLoss:{train_loss:.4f} TrainAcc:{train_acc*100:.2f}%  ValLoss:{val_loss:.4f} ValAcc:{val_acc*100:.2f}%\")\n",
    "\n",
    "    # Save best\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "        }, best_model_path)\n",
    "        print(\"  Saved best model ->\", best_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c72198-63ba-471f-b25c-7ce199af597d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5 -- load best model & evaluate on CASIA test (processed_casia/test should be ready)\n",
    "import torch\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# load best model\n",
    "ck = torch.load(best_model_path, map_location=device)\n",
    "model.load_state_dict(ck['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "# CASIA processed test path (if you used earlier 'processed_casia' path)\n",
    "casia_test_path = os.path.join(casia_base, \"processed_casia\", \"test\")  # update if different\n",
    "if not os.path.isdir(casia_test_path):\n",
    "    print(\"Warning: CASIA processed test path not found:\", casia_test_path)\n",
    "else:\n",
    "    # create DataLoader for CASIA test using FaceFolderDataset but val_transform + mtcnn\n",
    "    casia_test_dataset = FaceFolderDataset(casia_test_path, transform=val_transform, mtcnn=mtcnn)\n",
    "    casia_loader = DataLoader(casia_test_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in casia_loader:\n",
    "            imgs = imgs.to(device)\n",
    "            out = model(imgs)\n",
    "            preds = out.argmax(dim=1).cpu().numpy().tolist()\n",
    "            y_pred.extend(preds)\n",
    "            y_true.extend(labels.numpy().tolist())\n",
    "\n",
    "    print(\"CASIA Test Accuracy: %.2f%%\" % (100.0 * (sum(1 for i,j in zip(y_true,y_pred) if i==j) / len(y_true))))\n",
    "    print(classification_report(y_true, y_pred, target_names=[\"real\",\"fake\"]))\n",
    "    print(\"Confusion matrix:\\n\", confusion_matrix(y_true, y_pred))\n",
    "\n",
    "# Optional: evaluate all images in a folder (external deepfakes)\n",
    "def predict_folder(folder_path):\n",
    "    preds = []\n",
    "    for f in os.listdir(folder_path):\n",
    "        if not f.lower().endswith(('.jpg','.png','.jpeg')):\n",
    "            continue\n",
    "        p = os.path.join(folder_path, f)\n",
    "        # get cropped face with MTCNN\n",
    "        try:\n",
    "            img = Image.open(p).convert(\"RGB\")\n",
    "        except:\n",
    "            continue\n",
    "        face = mtcnn(img)\n",
    "        if face is None:\n",
    "            # fallback to center crop + transform\n",
    "            w,h = img.size\n",
    "            min_side = min(w,h)\n",
    "            left = (w-min_side)//2\n",
    "            top = (h-min_side)//2\n",
    "            face_img = img.crop((left, top, left+min_side, top+min_side))\n",
    "            inp = val_transform(face_img).unsqueeze(0).to(device)\n",
    "        else:\n",
    "            face_img = transforms.ToPILImage()(face)\n",
    "            inp = val_transform(face_img).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            out = model(inp)\n",
    "            pred = out.argmax(dim=1).item()\n",
    "        preds.append((f, \"real\" if pred==0 else \"fake\"))\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201afddc-ed07-49dc-a356-eeeba8acb19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "# external_preds = predict_folder(r\"C:\\Users\\mujta\\OneDrive\\Desktop\\deepfake_test_folder\")\n",
    "# print(external_preds[:20])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
